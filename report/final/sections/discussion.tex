The experimentation and results yielded some useful findings regarding the work and its usefulness.

First, sending in raw face images alone without any uniformity in or designation of features (such as the head dimensions, eye sizes and locations, et cetera) will not necessarily yield similar results across multiple faces. There were some examples presented showing different eyes elongated for example. Given the networks are configured to flow from the output destination to the source pixel location there is no coherent way to indicate the location of features since those are not known until the deformation is performed. One possibility to look at is reversing the flow of information so that the source locations are input and the destination locations are output. This may cause some issues with sparsely populated image generation which, depending on the distortion, may be difficult or even impossible to fix with blending and interpolation techniques. Another possibly more practical method might be to detect the features in the face image then allow warping to be done on the features individually and recombined to form a face. This has the disadvantage of compartmentalizing individual components though.

Second, interesting results can be obtained with very little complexity but the results tend to converge to similar designs. Even a few hidden nodes can greatly increase the diversity in the results. But to get past this initial no diversity hump the boosting effort seemed fairly useful. Another option might have been to simply start the network off with a few hidden nodes but that approach requires some assumptions in determining what might compose a desirable network, a task that's fairly complicated even with few nodes. For example, is figure \ref{fig:color_extraction} desirable? One may suppose it depends largely on the application; for general face deformations it's probably not very interesting. It's difficult to deduce beforehand that two sine nodes and one linear won't make something interesting and it's difficult to deduce that they won't ever create something interesting. The other aspect of boosting that is of interest is the part played by the cross correlation. It seemed to perform well at discriminating close to duplicates but they had to be very close for it to be useful. This may very well not be the best similarity indication available. Even if it were there would need to be large speed improvements made before it could be considered practical. This may very well come out of simply moving it away from Python into C or C++.

Third, the technique as it is currently implemented may have limited practical use on real images or detailed drawings but it seems to have some clear applications in less detailed use cases. The cartoons generated, for instance, show some potential of the system to evolve cartoons and, if expanded into a three dimensional space, perhaps also structures. This could be an interesting addition if trying to generate unique user-generated data (in games, for example).